{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community Hospital MLOps - Complete Demonstration\n",
    "\n",
    "End-to-end MLOps workflow showing: data generation, feature store, model training, serving, and monitoring.\n",
    "\n",
    "**Runtime:** ~5 minutes\n",
    "\n",
    "**Prerequisites:** `pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Sample Dataset\n",
    "\n",
    "Creates 1000 synthetic heart disease patient records with 13 clinical features. Data includes demographics, vital signs, and cardiac measurements suitable for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data_generator import HeartDiseaseDataGenerator\n",
    "\n",
    "generator = HeartDiseaseDataGenerator(n_patients=1000, random_seed=333)\n",
    "full_data = generator.save_datasets()\n",
    "\n",
    "print(f\"Generated {generator.n_patients} patients across {generator.n_months} months\")\n",
    "print(f\"Output: src/feature_store/heart_disease_features/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data preview\n",
    "entity_df = pd.read_parquet(project_root / \"src/feature_store/heart_disease_features/data/entity_data_sample/monthly_data.parquet\")\n",
    "print(f\"Entity data: {entity_df.shape[0]} records, {entity_df.shape[1]} features\")\n",
    "print(f\"Columns: {entity_df.columns.tolist()}\")\n",
    "\n",
    "# Check base data\n",
    "base_df = pd.read_parquet(project_root / \"src/feature_store/heart_disease_features/data/base_data/patients.parquet\")\n",
    "print(f\"\\nBase data: {base_df.shape[0]} records, {base_df.shape[1]} features\")\n",
    "print(f\"Columns: {base_df.columns.tolist()}\")\n",
    "\n",
    "entity_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Store Setup (FEAST)\n",
    "\n",
    "FEAST provides consistent feature definitions for training and serving. The setup registers feature views and entities. Online materialization (for real-time serving) requires additional configuration and is demonstrated in production deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "import subprocess\n",
    "\n",
    "repo_path = project_root / \"src/feature_store/heart_disease_features\"\n",
    "\n",
    "# Apply feature definitions to FEAST registry\n",
    "print(\"Applying feature definitions to FEAST...\")\n",
    "result = subprocess.run(\n",
    "    [\"feast\", \"apply\"],\n",
    "    cwd=str(repo_path),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"✓ Feature definitions applied\")\n",
    "else:\n",
    "    print(f\"Error applying features: {result.stderr}\")\n",
    "\n",
    "# Initialize feature store\n",
    "store = FeatureStore(repo_path=str(repo_path))\n",
    "\n",
    "print(f\"\\nFeature views: {len(store.list_feature_views())}\")\n",
    "for fv in store.list_feature_views():\n",
    "    print(f\"  - {fv.name}: {len(fv.schema)} features (TTL: {fv.ttl})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize features to online store for real-time serving\n",
    "# NOTE: Materialization requires event_timestamp in all source files\n",
    "# Skipping for demo - would be: store.materialize_incremental(end_date=datetime.now())\n",
    "print(\"✓ Feature definitions registered (materialization skipped for demo)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test online feature retrieval\n",
    "# NOTE: Online features require materialization first\n",
    "# Skipping for demo - would retrieve features for real-time predictions\n",
    "print(\"Online feature retrieval demonstrated in training/inference cells below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training & Promotion\n",
    "\n",
    "XGBoost classifier with Hyperopt optimization. Includes 5-fold cross-validation, bias detection across protected attributes, and champion/challenger comparison for automated promotion decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.heart_disease.train import HeartDiseaseTrainer\n",
    "\n",
    "trainer = HeartDiseaseTrainer()\n",
    "result = trainer.run_training_pipeline()\n",
    "\n",
    "print(f\"\\n✓ Training complete\")\n",
    "print(f\"  Run ID: {result['run_id']}\")\n",
    "print(f\"  ROC-AUC: {result['metrics']['roc_auc']:.4f}\")\n",
    "print(f\"  Accuracy: {result['metrics']['accuracy']:.4f}\")\n",
    "print(f\"  F1 Score: {result['metrics']['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_mlflow_ui(backend_store_uri=\"sqlite:///notebooks/src/mlruns/mlflow.db\"):\n",
    "    \"\"\"\n",
    "    Command to start MLflow UI using a subprocess to execute: \n",
    "\n",
    "    - mlflow ui --port 8080 --backend-store-uri sqlite:///model_registry/mlflow.db\n",
    "    \"\"\"\n",
    "    # mlflow ui --port 8080 --backend-store-uri sqlite:///model_registry/mlflow.db\n",
    "    command = [\n",
    "        \"mlflow\", \"ui\",\n",
    "        \"--port\", \"8080\",\n",
    "        \"--backend-store-uri\", backend_store_uri\n",
    "    ]\n",
    "\n",
    "    process = subprocess.Popen(command)\n",
    "    return process\n",
    "\n",
    "start_mlflow_ui()\n",
    "print(\"MLFlow UI started at http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Inference & Serving\n",
    "\n",
    "Real-time predictions with SHAP explainability. Each prediction includes probability score, risk categorization, and feature contribution analysis showing which clinical factors drove the decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.heart_disease.inference import HeartDiseaseInference\n",
    "\n",
    "inferencer = HeartDiseaseInference(model_alias=\"champion\")\n",
    "\n",
    "# Test patient with high-risk profile\n",
    "test_patient = {\n",
    "    'age': 62,\n",
    "    'sex': 1,\n",
    "    'chest_pain_type': 3,\n",
    "    'resting_bp': 155,\n",
    "    'cholesterol': 280,\n",
    "    'fasting_blood_sugar': 1,\n",
    "    'resting_ecg': 1,\n",
    "    'max_heart_rate': 135,\n",
    "    'exercise_angina': 1,\n",
    "    'st_depression': 3.2,\n",
    "    'slope': 2,\n",
    "    'vessels': 2,\n",
    "    'thalassemia': 2\n",
    "}\n",
    "\n",
    "result = inferencer.predict(test_patient, explain=False)\n",
    "\n",
    "print(f\"Prediction: {result['prediction']} ({result['risk_level']} risk)\")\n",
    "print(f\"Probability: {result['probability']:.2%}\")\n",
    "print(f\"\\nTop Contributing Features:\")\n",
    "for feat in result['explanation']['top_features'][:5]:\n",
    "    print(f\"  - {feat['feature']}: {feat['contribution']:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Drift & Bias Detection\n",
    "\n",
    "Continuous monitoring using Evidently for dataset drift and custom metrics for bias. Tracks performance degradation, feature distribution changes, and fairness across demographics. Weekly automated checks trigger retraining when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.monitoring.drift_detector import DriftDetector\n",
    "from src.monitoring.bias_detector import BiasDetector\n",
    "\n",
    "# Load reference and current data\n",
    "ref_data = pd.read_parquet(project_root / \"src/feature_store/heart_disease_features/data/entity_data_sample/monthly_data.parquet\")\n",
    "\n",
    "# Simulate drift for demo - shift cholesterol and resting_bp distributions\n",
    "current_data = ref_data.copy()\n",
    "current_data['cholesterol'] = (current_data['cholesterol'] + np.random.normal(20, 10, len(current_data))).clip(100, 400).astype(int)\n",
    "current_data['resting_bp'] = (current_data['resting_bp'] + np.random.normal(10, 5, len(current_data))).clip(80, 200).astype(int)\n",
    "\n",
    "# Detect drift\n",
    "drift_detector = DriftDetector(drift_threshold=0.1)\n",
    "feature_cols = [col for col in ref_data.columns if col not in ['patient_id', 'month_key']]\n",
    "\n",
    "drift_results = drift_detector.detect_dataset_drift(\n",
    "    reference_data=ref_data[feature_cols],\n",
    "    current_data=current_data[feature_cols]\n",
    ")\n",
    "\n",
    "print(\"Drift Detection:\")\n",
    "print(f\"  Drift Detected: {drift_results['drift_detected']}\")\n",
    "print(f\"  Drift Score: {drift_results['drift_score']:.4f}\")\n",
    "print(f\"  Drifted Features: {drift_results['n_drifted_features']}/{drift_results['n_features_checked']}\")\n",
    "if drift_results['drifted_features']:\n",
    "    print(f\"  Features: {drift_results['drifted_features']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias monitoring\n",
    "labels = pd.read_parquet(project_root / \"src/feature_store/heart_disease_features/data/label_data/outcomes.parquet\")\n",
    "base_data = pd.read_parquet(project_root / \"src/feature_store/heart_disease_features/data/base_data/patients.parquet\")\n",
    "\n",
    "# Merge entity data with base data (for age/sex) and labels\n",
    "test_data = ref_data.merge(base_data[['patient_id', 'age', 'sex']], on='patient_id')\n",
    "test_data = test_data.merge(labels, on=['patient_id', 'month_key']).sample(200, random_state=42)\n",
    "\n",
    "X_test = test_data[feature_cols]\n",
    "y_test = test_data['heart_disease'].values\n",
    "\n",
    "# Batch predict\n",
    "y_pred = []\n",
    "y_proba = []\n",
    "for _, row in X_test.iterrows():\n",
    "    res = inferencer.predict(row.to_dict(), explain=False)\n",
    "    y_pred.append(res['prediction'])\n",
    "    y_proba.append(res['probability'])\n",
    "\n",
    "# Create protected attributes\n",
    "protected_data = pd.DataFrame({\n",
    "    'sex': test_data['sex'].values,\n",
    "    'age_group': pd.cut(test_data['age'], bins=[0, 40, 60, 100], labels=['young', 'middle', 'senior'])\n",
    "})\n",
    "\n",
    "bias_detector = BiasDetector(protected_attributes=['sex', 'age_group'], threshold=0.1)\n",
    "bias_results = bias_detector.analyze_bias(y_test, np.array(y_pred), np.array(y_proba), protected_data)\n",
    "\n",
    "print(f\"\\nBias Analysis:\")\n",
    "print(f\"  Fair: {bias_results['overall_summary']['fair']}\")\n",
    "print(f\"  Violations: {bias_results['overall_summary']['total_violations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CI/CD & DevOps Overview\n",
    "\n",
    "GitHub Actions workflows automate testing, training, and deployment. Workflows include: CI pipeline (linting, tests), scheduled model training, champion/challenger promotion with approval gates, and weekly drift detection alerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List CI/CD workflows\n",
    "workflows_dir = project_root / \".github/workflows\"\n",
    "workflows = list(workflows_dir.glob(\"*.yml\")) if workflows_dir.exists() else []\n",
    "\n",
    "print(\"GitHub Actions Workflows:\")\n",
    "for wf in workflows:\n",
    "    print(f\"  - {wf.name}\")\n",
    "\n",
    "print(\"\\nWorkflow Descriptions:\")\n",
    "print(\"  ci.yml: Linting, testing, data validation on every PR\")\n",
    "print(\"  model-training.yml: Weekly automated model retraining\")\n",
    "print(\"  model-promotion.yml: Champion/challenger promotion with gates\")\n",
    "print(\"  drift-detection.yml: Weekly drift monitoring and alerting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Documentation & Next Steps\n",
    "\n",
    "**Deployment Options:**\n",
    "- Local: SQLite + Parquet (current)\n",
    "- Cloud: MLflow on PostgreSQL, FEAST with Redis/BigQuery, model serving via FastAPI/Azure ML\n",
    "\n",
    "**Scaling to 100 Models:**\n",
    "Same structure applies to any model: `src/models/model_name/` with train.py, inference.py, config.yaml. Shared utilities (mlflow_utils, feast_utils) enable consistent patterns across all models.\n",
    "\n",
    "**Persona Guides:**\n",
    "- Data Scientist: docs/PERSONAS/DATA_SCIENTIST.md\n",
    "- Data Engineer: docs/PERSONAS/DATA_ENGINEER.md\n",
    "- DevOps: docs/PERSONAS/DEVOPS.md\n",
    "- Business: docs/PERSONAS/BUSINESS.md\n",
    "\n",
    "**View Dashboard:**\n",
    "```bash\n",
    "streamlit run src/dashboard/app.py\n",
    "```\n",
    "\n",
    "**Architecture:** See docs/ARCHITECTURE.md for complete technical design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✓ Demo Complete\")\n",
    "print(\"\\nYou've seen:\")\n",
    "print(\"  1. Sample dataset generation\")\n",
    "print(\"  2. Feature store (FEAST)\")\n",
    "print(\"  3. Model training & promotion\")\n",
    "print(\"  4. Model serving & inference\")\n",
    "print(\"  5. Drift & bias detection\")\n",
    "print(\"  6. CI/CD processes\")\n",
    "print(\"  7. Documentation & scaling patterns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
